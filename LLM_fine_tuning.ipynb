{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Lo2a4N2gr1DT"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ChartQALLMDataset(Dataset):\n",
        "    def __init__(self, jsonl_path, tokenizer, max_len=1024):\n",
        "        self.samples = []\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                self.samples.append(json.loads(line))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "\n",
        "        prompt = (\n",
        "            \"You are a ChartQA model. Use the chart description and data table to answer.\\n\\n\"\n",
        "            f\"[DESCRIPTION]\\n{s['chart_description']}\\n\\n\"\n",
        "            f\"[DATA TABLE]\\n{s['data_points']}\\n\\n\"\n",
        "            f\"[QUESTION]\\n{s['instruction']}\\n\\n\"\n",
        "            \"[ANSWER]\\n\"\n",
        "        )\n",
        "\n",
        "        full_text = prompt + s[\"output\"]\n",
        "\n",
        "        enc = self.tokenizer(\n",
        "            full_text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        # Mask prompt tokens so loss is only on the answer\n",
        "        labels = enc[\"input_ids\"].clone()\n",
        "        answer_start = len(self.tokenizer(prompt).input_ids)\n",
        "        labels[:answer_start] = -100\n",
        "\n",
        "        enc[\"labels\"] = labels\n",
        "        return {k: v.squeeze(0) for k, v in enc.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hf-transfer\n",
        "!export HF_HUB_ENABLE_HF_TRANSFER=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1fUNYqw4o26",
        "outputId": "3e1399c0-0308-44c0-be71-a1a93db97deb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hf-transfer in /usr/local/lib/python3.12/dist-packages (0.1.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "MODEL_NAME = \"sshleifer/tiny-gpt2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n"
      ],
      "metadata": {
        "id": "zh9LfY5uukhp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34lW_Qqm-krM",
        "outputId": "66445fe5-762d-4ec2-99fc-8282e183a586"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50258, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1TlzCEnuDzP",
        "outputId": "cea3f396-849a-4acb-8287-18b04f89752f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/llm_train.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "  f.write('{\"instruction\": \"What was the highest sales month?\", \"chart_description\": \"A line chart showing monthly sales from January to June with a steady upward trend.\", \"data_points\": \"Month,Sales Jan,120 Feb,150 Mar,170 Apr,200 May,240 Jun,300\", \"output\": \"June had the highest sales with 300 units.\"}\\n{\"instruction\": \"Which country has the largest market share?\", \"chart_description\": \"A pie chart showing telecom market shares for four companies.\", \"data_points\": \"Country,Market Share China Telecom,53% China Unicom,34.2% China Mobile,6.2% Other,6.6%\", \"output\": \"China Telecom has the largest market share at 53%.\"}\\n{\"instruction\": \"How many countries exceed 20 units?\", \"chart_description\": \"A bar chart comparing values for nine countries across one date.\", \"data_points\": \"Country,Value Brazil,0 Mexico,0 Russia,0 India,0 Indonesia,0 Italy,4 France,0 United States,1 United Kingdom,0\", \"output\": \"Only Italy exceeds 20 units? No â€” none exceed 20 units; all values are below 20.\"}')"
      ],
      "metadata": {
        "id": "8nptif2I5ces"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "YhHGGk2L9y8R"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "train_dataset = ChartQALLMDataset(\"/content/llm_train.jsonl\", tokenizer)\n",
        "val_dataset = ChartQALLMDataset(\"/content/llm_train.jsonl\", tokenizer)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./chart_llm_lora\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=2,\n",
        "    bf16=torch.cuda.is_available(),\n",
        "    logging_steps=20,\n",
        "    save_steps=500,\n",
        "    eval_steps=200,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"./chart_llm_lora\")\n",
        "tokenizer.save_pretrained(\"./chart_llm_lora\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "cI5AuPhDw5wU",
        "outputId": "940b168e-510f-401f-b1de-f190279813c8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:02, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./chart_llm_lora/tokenizer_config.json',\n",
              " './chart_llm_lora/special_tokens_map.json',\n",
              " './chart_llm_lora/vocab.json',\n",
              " './chart_llm_lora/merges.txt',\n",
              " './chart_llm_lora/added_tokens.json',\n",
              " './chart_llm_lora/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}